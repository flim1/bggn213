---
title: "Class 9 Unsupervised Learning Mini-Project"
author: "Fabian Lim"
date: "10/30/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data Input

```{r}
wisc.df <- read.csv("WisconsinCancer.csv")
```

```{r}
head(wisc.df)
```

```{r}
class(wisc.df)
```

Here we examine data from `r nrow(wisc.df)` patient samples

```{r}
x <- table(wisc.df$diagnosis)
x
```

In this data-set we have `r x["M"]` cancer and `r x["B"]` non cancer patients.


```{r}
# Convert the features of the data: wisc.data
wisc.data <- as.matrix(wisc.df[,3:32])
```

```{r}
colnames(wisc.df)
```

```{r}
grep("_mean", colnames(wisc.df), value=TRUE)
```

To find out how many there are I can call `length()` on the result of `grep()`

```{r}
length(grep("_mean", colnames(wisc.df), value=TRUE))
```

## Principal Component Analysis
The next step in your analysis is to perform principal component analysis (PCA) on wisc.data.


```{r}
round(apply(wisc.data,2,sd), 3)
```
Apply function uses column if input 2 as argument (1 for rows)

Looks like we need to use `scale=TRUE` here as our data are all over the shop... (because the sd of the means are very different, hence we could mislead data analysis for thinking that those with large sd are more important)

```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp(wisc.data, scale=TRUE)
summary(wisc.pr)
```

Plot PC1 vs PC2 and color by M/B cancer/non-cancer diagnosis

```{r}
# without color
plot(wisc.pr$x[,1], wisc.pr$x[,2])
```

```{r}
# with color
plot(wisc.pr$x[,1:2], col=wisc.df$diagnosis)
```

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,3], col=wisc.df$diagnosis)
```

```{r}
x <- summary(wisc.pr)
```

```{r}
x$importance[,"PC1"]
```

The first PC captures `r x$importance[2,"PC1"]*100` of the original variance in the dataset.

Q8. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

```{r}
which(x$importance[3,] >0.7)[1]
```

## Hierarchical clustering of PCs

```{r}
data.scaled <- scale(wisc.data)
wisc.hclust <- hclust(dist(data.scaled))
plot (wisc.hclust)
abline(h=19, col="red")
```

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
table(wisc.hclust.clusters, wisc.df$diagnosis)
```

## Combining methods
Minimum number of principal components required to describe at least 90% of variability is 7 PCs
```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:7]), method="ward.D2")
plot(wisc.pr.hclust)
```

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
table(grps, wisc.df$diagnosis)
```

```{r}
plot(wisc.pr$x[,1:2], col=grps)
```

```{r}
plot(wisc.pr$x[,1:2], col=wisc.df$diagnosis)
```

```{r}
g <- as.factor(grps)
levels(g)
```

```{r}
g <- relevel(g,2)
levels(g)
```

```{r}
# Plot using our re-ordered factor
plot(wisc.pr$x[,1:2], col=g)
```

```{r eval=FALSE}
library(rgl)
plot3d(wisc.pr$x[,1:3], xlab="PC 1", ylab="PC 2", zlab="PC 3", cex=1.5, size=1, type="s", col=grps)
rglwidget(width = 400, height = 400)
```

```{r}
## Use the distance along the first 7 PCs for clustering i.e. wisc.pr$x[, 1:7]
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method="ward.D2")
```

```{r}
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
```

```{r}
# Compare to actual diagnoses
table(wisc.pr.hclust.clusters, wisc.df$diagnosis)
```

## Prediction

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

```{r}
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

